{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evo 2 Yarrowia Sparse Auto Encoder (Local Version)\n",
    "\n",
    "This notebook shows a **minimal, classâ€‘free** workflow to:\n",
    "\n",
    "1. **Load Yarrowia** sequence from a local FASTA (`GCF_001761485.1_ASM176148v1_genomic.fna`) and annotations from a local **GFF** (`genomic.gff`).\n",
    "2. Use a **local Evo2 model** (via `transformers`) to get layer activations.\n",
    "3. Load a preâ€‘trained **Topâ€‘K tied Sparse Autoencoder (SAE)** from Hugging Face.\n",
    "4. **Project activations into SAE features** and **plot a handful** of them with GFF annotations.\n",
    "\n",
    "> [!TIP]\n",
    "> **Setup:** Install dependencies with `pip install -U transformers huggingface_hub evo2 biopython flash-attn` and restart your kernel.\n",
    "> This notebook defaults to the **7B model** which is compatible with the provided SAE weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf97a546",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e2ee50a",
   "metadata": {},
   "source": [
    "### Set up imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92c51bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io, base64, json, zipfile, time, yaml, pkgutil, gc, os\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from Bio import SeqIO\n",
    "# from huggingface_hub import hf_hub_download\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "os.environ['LD_LIBRARY_PATH'] = f\"{os.environ.get('CUDA_HOME', '/opt/apps/software/system/CUDA/12.2.0')}/lib64:/.singularity.d/libs:\" + os.environ.get('LD_LIBRARY_PATH', '')\n",
    "from evo2 import Evo2\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# Reproducibility & device\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "dtype = torch.bfloat16  # works well for large tensors\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23607222",
   "metadata": {},
   "source": [
    "### Load Evo2 API helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5f9aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_CONFIG: Change to \"arcinstitute/evo2_40b\" if you have a compatible SAE model.\n",
    "MODEL_ID = \"arcinstitute/evo2_7b\" \n",
    "\n",
    "def evo2_forward(sequence, output_layers, model=MODEL_ID, device=device, dtype=dtype):\n",
    "    \"\"\"Local inference for Evo2 to get layer activations.\n",
    "    Returns (logits, acts_dict) as numpy arrays.\n",
    "    \"\"\"\n",
    "    global _local_model, _local_tokenizer\n",
    "    if '_local_model' not in globals():\n",
    "        print(f\"Loading model {model}... \")\n",
    "        _local_tokenizer = AutoTokenizer.from_pretrained(model, trust_remote_code=True)\n",
    "        if device == 'cuda':\n",
    "            _local_model = AutoModelForCausalLM.from_pretrained(\n",
    "                model, \n",
    "                trust_remote_code=True, \n",
    "                torch_dtype=dtype,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "        else:\n",
    "            _local_model = AutoModelForCausalLM.from_pretrained(\n",
    "                model, \n",
    "                trust_remote_code=True, \n",
    "                torch_dtype=dtype\n",
    "            ).to(device)\n",
    "        _local_model.eval()\n",
    "\n",
    "    inputs = _local_tokenizer(sequence, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    acts = {}\n",
    "    hooks = []\n",
    "    \n",
    "    def get_hook(name):\n",
    "        def hook(module, input, output):\n",
    "            data = output[0] if isinstance(output, tuple) else output\n",
    "            acts[f\"{name}.output\"] = data.detach().cpu().numpy()\n",
    "        return hook\n",
    "\n",
    "    # Register hooks for requested layers\n",
    "    for layer_name in output_layers:\n",
    "        try:\n",
    "            module = _local_model\n",
    "            if hasattr(module, 'backbone'):\n",
    "                module = module.backbone\n",
    "            elif hasattr(module, 'model'):\n",
    "                module = module.model\n",
    "                \n",
    "            parts = layer_name.split('.')\n",
    "            for part in parts:\n",
    "                if part.isdigit():\n",
    "                    module = module[int(part)]\n",
    "                else:\n",
    "                    module = getattr(module, part)\n",
    "            hooks.append(module.register_forward_hook(get_hook(layer_name)))\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not register hook for {layer_name}: {e}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = _local_model(**inputs)\n",
    "        lgt = outputs.logits.detach().cpu().numpy()\n",
    "\n",
    "    # Remove hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "        \n",
    "    def squeeze(x):\n",
    "        return x[0] if (isinstance(x, np.ndarray) and x.ndim >= 3 and x.shape[0] == 1) else x\n",
    "    \n",
    "    return squeeze(lgt), {k: squeeze(v) for k, v in acts.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c42f116",
   "metadata": {},
   "source": [
    "### Load Yarrowia sequence (first 25,000 bases of chromosome 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f73c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "FASTA_PATH = \"/home/jacobbw/Desktop/Code/yarrowia_language_modeling/yarrowia_notebooks/sparce_autoencoder/GCF_001761485.1_ASM176148v1_genomic.fna\"   # FASTA\n",
    "GFF_PATH   = \"genomic.gff\"                               # GFF annotations\n",
    "\n",
    "WINDOW_START = 1            # 1-based\n",
    "WINDOW_END   = 25_000       # inclusive 1..25000\n",
    "\n",
    "def pick_chr1_record(fasta_path):\n",
    "    \"\"\"Return the SeqRecord for chromosome 1 (best effort).\n",
    "    If not found, return the first record.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    for rec in SeqIO.parse(fasta_path, \"fasta\"):\n",
    "        rid = (rec.id + \" \" + (rec.description or \"\")).lower()\n",
    "        candidates.append(rec)\n",
    "        if any(tag in rid for tag in [\"chromosome 1\", \"chromosome i\", \"chr1\", \"chr 1\"]):\n",
    "            return rec\n",
    "    # fallback: first record\n",
    "    return candidates[0]\n",
    "\n",
    "chr1_rec = pick_chr1_record(FASTA_PATH)\n",
    "seq_1based = chr1_rec.seq[WINDOW_START-1:WINDOW_END]\n",
    "sequence = str(seq_1based)\n",
    "print(chr1_rec.id, \"length:\", len(chr1_rec.seq))\n",
    "print(\"Window length:\", len(sequence))\n",
    "print(sequence[:80] + \" ...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740fb299",
   "metadata": {},
   "source": [
    "### Ask Evo2 for layer activations over this sequence\n",
    "Potentially, we can look into trying other layer outputs to find different features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ebac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable TF32 for max precision, or enable for speed (your call)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "# === CONFIG ===\n",
    "CHECKPOINT_PATH = \"/home/jacobbw/roell_group/hf_cache/evo2_40b.pt\"\n",
    "SAE_LAYER_NAME = \"blocks.26.mlp.l3\"\n",
    "WANTED_LAYERS = [SAE_LAYER_NAME]\n",
    "USE_MULTI_GPU = True  # Set False if you want single GPU\n",
    "COMPILE_MODEL = True  # torch.compile for faster inference\n",
    "# === IMPORTS ===\n",
    "from evo2.models import Evo2\n",
    "from vortex.model.model import StripedHyena\n",
    "def nuke_fp8(model):\n",
    "    \"\"\"Aggressively remove all FP8 artifacts from model.\"\"\"\n",
    "    for module in model.modules():\n",
    "        # Kill fp8_meta dict\n",
    "        if hasattr(module, 'fp8_meta'):\n",
    "            module.fp8_meta = None\n",
    "        # Kill fp8 flag\n",
    "        if hasattr(module, 'fp8'):\n",
    "            module.fp8 = False\n",
    "        # Kill any TE-specific hooks that might interfere\n",
    "        if hasattr(module, '_load_from_state_dict'):\n",
    "            # Replace with vanilla PyTorch impl to skip FP8 restoration\n",
    "            module._load_from_state_dict = partial(\n",
    "                torch.nn.Module._load_from_state_dict, module\n",
    "            )\n",
    "def load_state_dict_streaming(model, state_dict, device):\n",
    "    \"\"\"\n",
    "    Load weights one-by-one directly to device.\n",
    "    Avoids holding two copies in memory.\n",
    "    \"\"\"\n",
    "    model_state = model.state_dict()\n",
    "    \n",
    "    for key in model_state.keys():\n",
    "        if key in state_dict:\n",
    "            # Stream directly: disk -> GPU (via mmap)\n",
    "            param = state_dict[key]\n",
    "            if param.dtype != torch.bfloat16:\n",
    "                param = param.to(dtype=torch.bfloat16)\n",
    "            model_state[key].copy_(param.to(device=device, non_blocking=True))\n",
    "    \n",
    "    # Sync to ensure all transfers complete\n",
    "    torch.cuda.synchronize()\n",
    "def load_evo2_blazing_fast():\n",
    "    \"\"\"\n",
    "    Maximum performance loader for Evo2 40B on H200s.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ðŸš€ BLAZING FAST EVO2 LOADER\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Verify hardware\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Found {num_gpus} GPU(s): {[torch.cuda.get_device_name(i) for i in range(num_gpus)]}\")\n",
    "    \n",
    "    primary_device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(primary_device)\n",
    "    \n",
    "    # â”€â”€â”€ 1. CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\n[1/5] Loading config...\")\n",
    "    try:\n",
    "        config_data = pkgutil.get_data(\"evo2\", \"configs/evo2_40b.yml\")\n",
    "        if config_data is None:\n",
    "            config_data = pkgutil.get_data(\"evo2.models\", \"configs/evo2_40b.yml\")\n",
    "        config = yaml.safe_load(config_data)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Config load failed: {e}. Check evo2 package installation.\")\n",
    "    # â”€â”€â”€ 2. INIT MODEL ON GPU (ZERO RAM) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\n[2/5] Initializing model skeleton on GPU...\")\n",
    "    \n",
    "    # Use meta device first for zero-allocation init, then materialize on GPU\n",
    "    # This is the fastest way to create a model skeleton\n",
    "    with torch.device('meta'):\n",
    "        model = StripedHyena(config)\n",
    "    \n",
    "    # Now materialize on GPU with empty tensors\n",
    "    def materialize_to_device(module, device, dtype):\n",
    "        for name, param in module._parameters.items():\n",
    "            if param is not None:\n",
    "                module._parameters[name] = torch.nn.Parameter(\n",
    "                    torch.empty(param.shape, device=device, dtype=dtype),\n",
    "                    requires_grad=False\n",
    "                )\n",
    "        for name, buf in module._buffers.items():\n",
    "            if buf is not None:\n",
    "                module._buffers[name] = torch.empty(buf.shape, device=device, dtype=dtype)\n",
    "    \n",
    "    for module in model.modules():\n",
    "        materialize_to_device(module, primary_device, torch.bfloat16)\n",
    "    \n",
    "    print(f\"   Model structure allocated: {torch.cuda.memory_allocated(0)/1024**3:.2f} GB\")\n",
    "    \n",
    "    # â”€â”€â”€ 3. NUKE FP8 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\n[3/5] Patching FP8...\")\n",
    "    nuke_fp8(model)\n",
    "    \n",
    "    # â”€â”€â”€ 4. STREAM WEIGHTS DIRECTLY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\n[4/5] Streaming weights from disk â†’ GPU...\")\n",
    "    \n",
    "    # Memory map the checkpoint (doesn't load into RAM)\n",
    "    state_dict = torch.load(CHECKPOINT_PATH, map_location=\"cpu\", mmap=True, weights_only=True)\n",
    "    if \"state_dict\" in state_dict:\n",
    "        state_dict = state_dict[\"state_dict\"]\n",
    "    \n",
    "    # Filter FP8 garbage\n",
    "    fp8_keys = [k for k in state_dict.keys() if \"_extra_state\" in k]\n",
    "    for k in fp8_keys:\n",
    "        del state_dict[k]\n",
    "    print(f\"   Filtered {len(fp8_keys)} FP8 metadata keys\")\n",
    "    \n",
    "    # Stream load (one tensor at a time: disk â†’ GPU)\n",
    "    load_state_dict_streaming(model, state_dict, primary_device)\n",
    "    \n",
    "    del state_dict\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"   Post-load VRAM: {torch.cuda.memory_allocated(0)/1024**3:.2f} GB\")\n",
    "    \n",
    "    # â”€â”€â”€ 5. MULTI-GPU SPLIT (OPTIONAL) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if USE_MULTI_GPU and num_gpus >= 2:\n",
    "        print(\"\\n[5/5] Splitting model across GPUs...\")\n",
    "        # Naive layer split: first half on GPU 0, second half on GPU 1\n",
    "        # For StripedHyena, layers are in model.blocks\n",
    "        if hasattr(model, 'blocks'):\n",
    "            n_layers = len(model.blocks)\n",
    "            split_point = n_layers // 2\n",
    "            for i, block in enumerate(model.blocks):\n",
    "                target_device = torch.device(f\"cuda:{0 if i < split_point else 1}\")\n",
    "                block.to(target_device)\n",
    "            print(f\"   Layers 0-{split_point-1} -> GPU 0\")\n",
    "            print(f\"   Layers {split_point}-{n_layers-1} -> GPU 1\")\n",
    "        else:\n",
    "            print(\"   Warning: 'blocks' attribute not found. Using single GPU.\")\n",
    "    else:\n",
    "        print(\"\\n[5/5] Single GPU mode - skipping split\")\n",
    "    \n",
    "    # â”€â”€â”€ 6. COMPILE (torch.compile for speed) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if COMPILE_MODEL:\n",
    "        print(\"\\n[BONUS] Compiling model with torch.compile...\")\n",
    "        try:\n",
    "            model = torch.compile(model, mode=\"reduce-overhead\", fullgraph=False)\n",
    "            print(\"   âœ“ Compiled successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âœ— Compile failed (non-fatal): {e}\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # â”€â”€â”€ WRAP IN EVO2 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    wrapper = Evo2.__new__(Evo2)\n",
    "    wrapper.model = model\n",
    "    wrapper.tokenizer = None  # Init separately if needed\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"âœ… MODEL READY\")\n",
    "    print(f\"   GPU 0: {torch.cuda.memory_allocated(0)/1024**3:.2f} GB\")\n",
    "    if num_gpus >= 2:\n",
    "        print(f\"   GPU 1: {torch.cuda.memory_allocated(1)/1024**3:.2f} GB\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return wrapper\n",
    "# === RUN ===\n",
    "model_obj = load_evo2_blazing_fast()\n",
    "# === INFERENCE ===\n",
    "# Warmup (first run is slower due to CUDA kernels)\n",
    "print(\"\\nWarmup run...\")\n",
    "with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "    # Dummy input for warmup\n",
    "    # Replace with actual sequence\n",
    "    _ = model_obj.model(torch.zeros(1, 1024, dtype=torch.long, device=\"cuda:0\"))\n",
    "print(\"\\nActual inference...\")\n",
    "with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "    logits, acts = evo2_forward(sequence, output_layers=WANTED_LAYERS, model=model_obj)\n",
    "print(\"Layers returned:\", list(acts.keys()))\n",
    "if f\"{SAE_LAYER_NAME}.output\" in acts:\n",
    "    print(\"Activations shape:\", acts[f\"{SAE_LAYER_NAME}.output\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f5a0e2",
   "metadata": {},
   "source": [
    "### Inspect the activation vectors at the output of layer 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9252c5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(layer_act)} tokens in this sequence window.')\n",
    "print(f'Each token has {len(layer_act[0])} features. This is the model hidden size or model dimension.')\n",
    "\n",
    "# make a dataframe of this data with columns: position, base, feature_0, feature_1, ..., feature_n\n",
    "df = pd.DataFrame(layer_act, columns=[f'feature_{i}' for i in range(layer_act.shape[1])])\n",
    "df.insert(0, 'base', list(sequence))\n",
    "df.insert(0, 'position', range(WINDOW_START, WINDOW_END + 1))\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7fbdfa",
   "metadata": {},
   "source": [
    "### Load the Topâ€‘K tied SAE model from from Hugging Face\n",
    "https://huggingface.co/Goodfire/Evo-2-Layer-26-Mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bedc105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll load the weights and keep them as plain tensors in a dict.\n",
    "# These weights were trained with expansion_factor=8 and k=64.\n",
    "SAE_REPO = \"Goodfire/Evo-2-Layer-26-Mixed\"\n",
    "SAE_FILE = \"sae-layer26-mixed-expansion_8-k_64.pt\"\n",
    "\n",
    "sae_path = hf_hub_download(repo_id=SAE_REPO, filename=SAE_FILE, repo_type=\"model\")\n",
    "state = torch.load(sae_path, map_location=\"cpu\", weights_only=True)\n",
    "\n",
    "# Normalize potential key prefixes (strip '_orig_mod.' or 'module.')\n",
    "clean = {}\n",
    "for k, v in state.items():\n",
    "    nk = k.replace(\"_orig_mod.\", \"\").replace(\"module.\", \"\")\n",
    "    clean[nk] = v\n",
    "\n",
    "# Expected keys: 'W', 'b_enc', 'b_dec'\n",
    "W     = clean[\"W\"].to(device=device, dtype=dtype)         # [d_in, d_hidden_expanded]\n",
    "b_enc = clean[\"b_enc\"].to(device=device, dtype=dtype)     # [d_hidden_expanded]\n",
    "b_dec = clean[\"b_dec\"].to(device=device, dtype=dtype)     # [d_in]\n",
    "TOP_K = 64\n",
    "\n",
    "print(\"W:\", tuple(W.shape))\n",
    "print(\"b_enc:\", tuple(b_enc.shape))\n",
    "print(\"b_dec:\", tuple(b_dec.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f24ae58",
   "metadata": {},
   "source": [
    "### Define function for running the model (encode/decode helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab36b218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return torch.maximum(x, torch.zeros_like(x))\n",
    "\n",
    "def encode_topk(x, W, b_enc, k=TOP_K):\n",
    "    \"\"\"x: [T, d_in]; returns sparse features f: [T, d_hidden_expanded] with Topâ€‘K per row.\"\"\"\n",
    "    # Pre-activation\n",
    "    f = x @ W + b_enc                  # [T, d_hidden_expanded]\n",
    "    f = relu(f)\n",
    "\n",
    "    # Topâ€‘K per row\n",
    "    # values, idx: [T, k]\n",
    "    values, idx = torch.topk(f, k=min(k, f.shape[-1]), dim=-1)\n",
    "    out = torch.zeros_like(f)\n",
    "    out.scatter_(dim=-1, index=idx, src=values)\n",
    "    return out\n",
    "\n",
    "def decode(f, W, b_dec):\n",
    "    return f @ W.T + b_dec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6855a3",
   "metadata": {},
   "source": [
    "### Project Evo2 activations through the SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead6be5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert layer_act (numpy) -> torch\n",
    "x = torch.tensor(layer_act, device=device, dtype=dtype)   # [T, d_in]\n",
    "\n",
    "print(f'The input to the SAE has shape: {x.shape}')\n",
    "print(f'The SAE weight (W) has shape: {W.shape}')\n",
    "\n",
    "if x.shape[1] != W.shape[0]:\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"DIMENSION MISMATCH ERROR\")\n",
    "    print(f\"Model activations (dim {x.shape[1]}) do not match SAE weights (dim {W.shape[0]}).\")\n",
    "    print(\"To fix this, ensure you are using 'arcinstitute/evo2_7b'.\")\n",
    "    print(\"=\"*40 + \"\\n\")\n",
    "else:\n",
    "    with torch.no_grad():\n",
    "        f_sparse = encode_topk(x, W, b_enc, k=TOP_K)          # [T, d_hidden_expanded]\n",
    "\n",
    "    # Convert sparse features to numpy for further analysis\n",
    "    sae_acts = f_sparse.float().cpu().numpy()\n",
    "    print(f'The sae activations have shape: {sae_acts.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9865a5",
   "metadata": {},
   "source": [
    "### Inspect the feats np object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10de69e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(sae_acts)} tokens in this sequence window.')\n",
    "print(f'Each token has {len(layer_act[0])} features. This is the model hidden size or model dimension.')\n",
    "print(f'Each token embedding was expanded to {sae_acts.shape[1]} features by the SAE.')\n",
    "\n",
    "# convert to a dataframe\n",
    "df_feats = pd.DataFrame(sae_acts, columns=[f'feature_{i}' for i in range(sae_acts.shape[1])])\n",
    "df_feats.insert(0, 'base', list(sequence))\n",
    "df_feats.insert(0, 'position', range(WINDOW_START, WINDOW_END + 1))\n",
    "df_feats.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb0d3ab",
   "metadata": {},
   "source": [
    "### Which features are most active?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba9674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average the features across all tokens,\n",
    "feature_only_df = df_feats[df_feats.columns[2:]]  # exclude position, base\n",
    "\n",
    "display(feature_only_df.head())\n",
    "\n",
    "# loop over the columns and count the number of non-zero entries per feature\n",
    "nonzero_counts = {}\n",
    "for col in feature_only_df.columns:\n",
    "    nonzero_counts[col] = (feature_only_df[col] != 0).sum()\n",
    "\n",
    "nonzero_counts_series = pd.Series(nonzero_counts)\n",
    "nonzero_counts_series.sort_values(ascending=False)[:20]  # show top 20 most active features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8b79ab",
   "metadata": {},
   "source": [
    "### Make a binary version of the feature data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d415534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a binary version of the feature data frame\n",
    "df_feats_binary = df_feats.copy()\n",
    "for col in df_feats_binary.columns[2:]:  # exclude position, base\n",
    "    df_feats_binary[col] = (df_feats_binary[col] != 0).astype(int)\n",
    "\n",
    "df_feats_binary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b7038d",
   "metadata": {},
   "source": [
    "### Parse GFF and collect annotations in the 1..25,000 window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90229bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal GFF parser: we keep rows overlapping our window on this chromosome.\n",
    "def parse_gff_window(gff_path, target_seqid, start_1b, end_1b,\n",
    "                     keep_types=None):\n",
    "    if keep_types is None:\n",
    "        keep_types = {\"gene\",\"CDS\",\"exon\",\"mRNA\",\"ncRNA\",\"tRNA\",\"rRNA\",\"misc_feature\",\"Regulatory\",\"tmRNA\",\"mobile_element\"}\n",
    "\n",
    "    out = []\n",
    "    with open(gff_path, \"r\", newline=\"\") as fh:\n",
    "        for line in fh:\n",
    "            if not line or line.startswith(\"#\"):\n",
    "                continue\n",
    "            parts = line.strip().split(\"\t\")\n",
    "            if len(parts) < 9:\n",
    "                continue\n",
    "            seqid, source, ftype, start, end, score, strand, phase, attrs = parts\n",
    "            if seqid != target_seqid:\n",
    "                continue\n",
    "            try:\n",
    "                s = int(start); e = int(end)\n",
    "            except:\n",
    "                continue\n",
    "            if e < start_1b or s > end_1b:\n",
    "                continue\n",
    "            if ftype not in keep_types:\n",
    "                continue\n",
    "            # Clip to window for plotting\n",
    "            s_clipped = max(start_1b, s) - start_1b\n",
    "            e_clipped = min(end_1b,   e) - start_1b\n",
    "            out.append((s_clipped, e_clipped, ftype, strand, attrs))\n",
    "    return out\n",
    "\n",
    "# Try to match the sequence/chromosome name used in GFF to our chosen FASTA record\n",
    "gff_seqid = chr1_rec.id\n",
    "ann = parse_gff_window(GFF_PATH, gff_seqid, WINDOW_START, WINDOW_END)\n",
    "\n",
    "print(\"Annotations kept:\", len(ann))\n",
    "print(\"First few:\", ann[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cedb872",
   "metadata": {},
   "source": [
    "### Inspect the annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6203e950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view annotations as a dataframe\n",
    "df_ann = pd.DataFrame(ann, columns=[\"start\", \"end\", \"type\", \"strand\", \"attributes\"])\n",
    "df_ann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93fd300",
   "metadata": {},
   "source": [
    "### Make a dataframe with columns for position, base, and each annotation as a binary value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6dfdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of possible annotation types\n",
    "annotation_types = df_ann[\"type\"].unique()\n",
    "\n",
    "print(\"Annotation types present in the sampled sequence:\", annotation_types)\n",
    "\n",
    "# make a dataframe of zeros with columns for each annotation type\n",
    "df_ann_binary = pd.DataFrame(0, index=df_feats_binary.index, columns=annotation_types)\n",
    "\n",
    "# loop over the rows and set the corresponding positions to 1\n",
    "for index, row in df_ann.iterrows():\n",
    "    df_ann_binary.loc[row[\"start\"]:row[\"end\"], row[\"type\"]] = 1\n",
    "df_ann_binary.insert(0, 'base', list(sequence))\n",
    "df_ann_binary.insert(0, 'position', range(WINDOW_START, WINDOW_END + 1))\n",
    "\n",
    "display(df_ann_binary.iloc[1510:1530])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5939a71b",
   "metadata": {},
   "source": [
    "### Add annotations to the df_feats_binary dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e718c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add annotations to the df_feats_binary dataframe\n",
    "df_feats_ann_binary = pd.concat([df_ann_binary, df_feats_binary], axis=1)\n",
    "\n",
    "df_feats_ann_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3676a9",
   "metadata": {},
   "source": [
    "### Analyze the correlations between the features and the annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de5c385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the annotation and feature columns\n",
    "annotation_cols = df_ann_binary.columns[2:]  # exclude position, base\n",
    "feature_cols = [col for col in df_feats_ann_binary.columns if col.startswith(\"feature_\")]\n",
    "\n",
    "# make simplified dataframes\n",
    "annotations = df_feats_ann_binary[annotation_cols]\n",
    "features = df_feats_ann_binary[feature_cols]\n",
    "\n",
    "# dictionary to hold correlation results\n",
    "correlation_results = {}\n",
    "\n",
    "# loop over the annotation columns and compute correlation with each feature\n",
    "for ann_col in annotation_cols:\n",
    "    corr = features.corrwith(annotations[ann_col])  # computes correlation column-wise\n",
    "    correlation_results[ann_col] = corr\n",
    "\n",
    "corr_df = pd.DataFrame(correlation_results)\n",
    "\n",
    "corr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce979fa",
   "metadata": {},
   "source": [
    "### Find the features with the highest correlations to each annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e60b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the features with the highest correlations to each annotation\n",
    "# loop over the annotation columns and get top correlated features\n",
    "corr_df['gene'].sort_values(ascending=False)[:5]  # top 5 features correlated with 'gene' annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a handful of SAE features with GFF overlays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e119ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f7be59",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose a few feature indices to visualize (change to taste)\n",
    "# selected_features = [15680, 28339, 1050, 25666]\n",
    "selected_features = [15809, 302, 7703, 26750, 25852]\n",
    "\n",
    "ANNOTATION_COLORS = {\n",
    "    'CDS': 'lightyellow',\n",
    "    'gene': 'lightgray', \n",
    "    'mobile_element': 'lightgreen',\n",
    "    'misc_feature': 'khaki',\n",
    "    'rRNA': '#7AC8AC',\n",
    "    'tRNA': '#662D91',\n",
    "    'ncRNA': 'white',\n",
    "    'Regulatory': 'lightcoral',\n",
    "    'tmRNA': 'salmon',\n",
    "    'exon': 'lightblue',\n",
    "    'mRNA': 'lavender'\n",
    "}\n",
    "\n",
    "T = sae_acts.shape[0]\n",
    "fig, axes = plt.subplots(len(selected_features), 1, figsize=(24, 1.6*len(selected_features)), sharex=True)\n",
    "\n",
    "for i, feat_id in enumerate(selected_features):\n",
    "    ax = axes[i] if len(selected_features) > 1 else axes\n",
    "    if feat_id >= sae_acts.shape[1]:\n",
    "        ax.text(0.01, 0.5, f\"Feature {feat_id} out of range\", transform=ax.transAxes)\n",
    "        continue\n",
    "    ax.plot(sae_acts[:, feat_id], lw=0.7, label=f\"feature {feat_id}\", alpha=0.9)\n",
    "    for s, e, ftype, strand, attrs in ann:\n",
    "        ax.axvspan(s, e, color=ANNOTATION_COLORS.get(ftype, 'lightgray'), alpha=0.25)\n",
    "    ax.set_xlim(0, T)\n",
    "    ax.set_yticks([0, max(0.1, sae_acts[:, feat_id].max())])\n",
    "    ax.legend(loc=\"upper right\", frameon=False)\n",
    "\n",
    "plt.xlabel(\"Position (bp) in 1..25,000 window\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Tips / Next steps\n",
    "- Change `SAE_LAYER_NAME` to probe different hidden layers.\n",
    "- Adjust `selected_features` to scan for interesting peaks.\n",
    "- Expand `WINDOW_END` or slide `WINDOW_START` to explore more of chromosome 1.\n",
    "- Save `feats_np` and overlay other tracks (GC%, motif hits, etc.).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
